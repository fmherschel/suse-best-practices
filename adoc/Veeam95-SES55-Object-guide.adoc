ifdef::env-daps[]
:imgpath:
endif::[]
ifndef::env-daps[]
:imgpath: ../images/src/png/
endif::[]
:SUSEProduct: SUSE Enterprise Storage
:SESversion: (v5.5)
:SLESversion: 12 SP3
:vendor: Veeam
:v: Veeam
:vplatform: Backup & Recovery
:vplatform1: Object Storage Repository
:vplatformver: 9.5
:vvalidationlink1: https://www.veeam.com/ready.html
:vvalidationlink2: https://www.veeam.com/ready.html

:docinfo:

= SUSE(R) Enterprise Storage Implementation Guide for {vendor} {vplatform}

''''
== Solution Components
SUSE::

{SUSEProduct} {SESversion}

{vendor}::

{vendor} {vplatform} {vplatformver}

== Introduction
The objective of this guide is to present a step-by-step guide on how to implement {SUSEProduct} {SESversion} with {vendor} {vplatform} as both a Linux Repository and an S3 target as part of a Scale Out Backup Repository.  It is suggested that the document be read in its entirety, along with the supplemental <<appendix>> information before attempting the process.

The deployment presented in this guide aligns with architectural best practices of both SUSE and {vendor}.

Upon completion of the steps in this document, a working SUSE Enterprise Storage {SESversion} cluster will be operational as described in the https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html[SUSE Enterprise Storage Deployment Guide] and integrated with {vendor} {vplatform}.

== Solution Description
The solution outlined in this guide enables a customer to deploy a disk-to-disk target that is orchestrated through Veeam. SUSE Enterprise Storage can be utilized as a backup target via a Veeam proxy over a common network, and the result is a high-performing and flexible backup target with exabyte scalability. 


== Business Value
*SUSE Enterprise Storage*

SUSE Enterprise Storage delivers  for a Veeam disk-to-disk backup solution with:

* Commodity hardware for minimal hardware cost
* Open source software, for minimal software cost and maximum flexibility
* A self-managing, self-healing architecture for minimal management cost
* A flexible, cluster-based design for graceful and inexpensive upgrade and innovative licensing model that avoids per-gigabyte storage charges, so you won’t owe more for saving more data.
* With minimal acquisition cost, management cost, and upgrade cost, SUSE Enterprise Storage is the lowest-price solution for enterprise archive and backup implementations

*{vendor} {vplatform}*

{vendor} {vplatform} delivers availability for all your virtual, physical and cloud-based workloads. Through a single management console, you can manage fast, flexible and reliable backup, recovery and replication of all your applications and data to eliminate legacy backup solutions forever. The solution includes native, certified SAP support for backups and recoveries. 

Together, {vendor} and SUSE deliver the flexibility and near-unlimited scalability you want for long-term data retention, plus a single storage architecture that delivers the various performance requirements a {vendor} backup solution needs. lt’s ideal for mission-critical applications and platforms such as SAP HANA, and allows you to recover vital data fast when failures occur.

*Solution Value Propositions*

* Issue: Customer needs to handle more simultaneous backup streams but not capacity +
Solution: Add more Veeam proxies and Linux repository servers connecting the existing SES environment

* Issue: Customer needs more storage, but not more simultaneous streams +
Solution: Add more OSDs to the SES cluster

* Issue: Customer wants to add S3 repository on-prem for long term archive +
Solution: Deploy RGWs for SES and implement the S3 repo

* Issue: Customer has multiple sites with small local Veeam repositories, but wants to replicate to a central Veeam location +
Solution: Deploy Veeam server and Linux repos with SES as central site OR deploy SES with S3 to act as central S3 repository

One of the big benefits is that every Veeam backup can benefit from the aggregated throughput of the cluster.  This brings both performance and storage efficiency. Instead of being limited to the throughput capacity of a single server, the I/O is spread across ALL the storage nodes.  It also means that there won’t be one storage enclosure that is maxed out on I/O capability while another is sitting idle.  This is all done without using Veeam’s Scale Out Backup Repository.﻿


== Requirements

The solution has the following requirements:

* Simple to setup and deploy and meets the documented guidelines for system hardware, networking and environmental prerequisites.
* Adaptable to the physical and logical constraints needed by the business, both initially and as needed over time for performance, security, and scalability concerns.
* Resilient to changes in physical infrastructure components, caused by failure or required maintenance.
* Capable of providing optimized object and block services to client access nodes, either directly or through gateway services.
* Data protection configurable to meet the customer's individual needs at a granular level. 
* Meet the Veeam Ready performance guidelines.

== Architectural overview
{SUSEProduct} provides everything that Veeam needs for storage, from the high-performance tier to the Cloud Tier repository.  

image::{imgpath}VeeamArchitecturewSES.png[Veeam and SES Architecture, scaledwidth=100%]

=== Solution Architecture - RBD
SUSE Enterprise Storage can be used as a storage location for Veeam via a Veeam Linux Repository. The architecture and settings described below were used during Veeam Ready testing to achieve the Repository level of validation.

The architecture used to achieve compliance with the {vvalidationlink1}[Veeam Ready program] utilizes a RADOS Block Device on a Veeam Linux Repository server. This paper will also briefly discuss CephFS as a potential storage mechanism, though it was not utilized for testing.

=== Solution Architecture - S3
{vendor} {vplatform} enables the usage of storage targets that are compatible with specific S3 API calls.  SUSE Enterprise Storage provides a target that meets these requirements and has passed through the {vvalidationlink2}[Veeam Ready validation testing for S3 Object target.]



=== Architectural Notes & Discussion
For both RBD and CephFS, having both the proxy and the Linux Repository server local to the ESX server offers several benefits:

.	The Veeam Proxy server located on the esx server is able to directly mount the VMware snapshot images, resulting in the highest possible streaming read performance for the backups. The figure below illustrates the traffic flow of a backup process.
.	Network communication from the Veeam Proxy to the Linux Repository server flows across the ESX server without traversing the physical network infrastructure. This results in very high network performance between these two critical pieces of infrastructure for the Veeam Backup and Restore environment.

image::{imgpath}VeeamDataFlow.png[Veeam Data Flow, scaledwidth=100%]

An optimal configuration for a large site would include Veeam Proxy and Linux Repository targets on each ESX host. This being not feasible for many customers, it is recommended that at least a Veeam Proxy be present where it is able to perform a vmware native mount of each system being backed up.

== Pool Configuration
When configuring the SUSE Enterprise Storage cluster for use as a backup target, the data protection scheme is an important consideration. There are two main options for data protection, each with advantages and disadvantages. 

The first is replication. It works by replicating each data chunk on each of the specified number of unique devices. The default is three. If the failure domain is assumed to be at the storage host level, this means the cluster could survive the loss of two storage servers without data loss. The downside of replication is the space overhead, which is 200% or two-thirds of the total cluster capacity. 

The performance characteristics of replication are that it has lower latency than erasure coding.  This is especially true where the I/O pattern is that of small random I/O. 

The second scheme is erasure coding (EC). It works by splitting the data into the specified number of chunks (k) and then performing a mathematical calculation to create the requested number of EC chunks (m). Again, assuming the failure domain is at the host level, a system using an EC scheme of k=6, m=3 has an overhead of only 50%, or one-third of the total cluster capacity. Because EC actually writes less data, it is sometimes faster than replication for writes, but slower on the reads due to the requirement to reassemble the data from multiple nodes.

Another aspect to consider is the total cluster size. In general, it is not recommended to use EC with a cluster of fewer than seven storage nodes. When using EC with SUSE Enterprise Storage, it is recommended that the data chunks + (2x erasure coding chunks) is less than or equal to the cluster node count. Expressed in a formula: 


[source]
data chunks [k] + (coding chunks [m] * 2) <= cluster node count


A cluster size of seven would thus allow for 3 data chunks + 2 erasure coding chunks + 2 spare nodes to allow for device failures. In a larger cluster, EC profiles of 8+3, 6+4, 9+3 and the like are not uncommon and represent superior percentages of storage available for data.

An additional consideration is the availability of hardware accelerators for erasure coding. Intel CPUs provide such an accelerator, which is specified with the plugin option when creating the erasure coding profile for the pool. 


[source]
ceph osd erasure-code-profile set veeam_ec plugin=isa k=8 m=3



=== Ceph protocol – RBD
The RBD protocol is the native block protocol for Ceph. Clients leveraging RBD could be termed “intelligent” because they are able to leverage the CRUSH algorithm to determine where data will be placed and thus communicate directly to each individual storage device. The result is performance that scales horizontally with the cluster. 

As a client protocol, RBD has numerous tuning options that can be controlled on each client, or for the cluster as a whole. These include things like caching type, size, etc. For this effort, some tuning was performed for the caching parameters to optimize performance for the I/O patterns being tested. These are outlined in the deployment section below.

The Veeam Linux Repository maps the RBD device created as a block device and then a file system is placed on it. This allows for tuning that can be applied to the particular filesystem you plan to use and to accelerate performance.
 
=== Ceph protocol – CephFS
While not explicitly tested for Veeam Ready performance testing, CephFS, the distributed file system, is available for use with Veeam as well. Anecdotal testing indicates performance of nearly the same level as RBD. An advantage of this particular protocol choice is that multiple repositories can be hosted on the same massively scaleable distributed file system. This also means that if a backup server disappears or fails, it is quite simple to add the repository to another server.

=== Ceph protocol - S3
The S3 protocol has become the de-facto standard for use in developing web-scale friendly applications that store and retrieve data.  The protocol uses either HTTP or HTTPS as the data transport protocol, making it capable of leveraging standard load-balancing and proxy technologies to ensure scalability and improved security. 

== Deployment Recommendations

This deployment section should be seen as a supplement to available online https://www.suse.com/documentation/[documentation.]  Specifically, the https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html[SUSE Enterprise Storage 5 Deployment Guide] as well as https://www.suse.com/documentation/sles-12/book_sle_admin/data/book_sle_admin.html[SUSE Linux Enterprise Server Administration Guide.] 

=== Network Deployment Overview
There are multiple considerations when working with a backup environment when it comes to designing the network to support horizontally scaling storage.  These include single stream throughput, aggregate write throughput, verification job requirements, and any replication traffic that may be needed.  It is important to identify the maximum simultaneous throughput that is required to support the bakcup traffic and then account for a back-end operation like reconstruction of a failed node.  

If two physically separate networks are utilized, it is somewhat simple to calculate and leave an appropriate amount of network bandwidth for back-end reconstruction for a replicated storage environment.
[.text-center, font-size:20em]
[source]
[back-end network throughput] = [front-end network] * 3

[.text-left]

Sizing the network in this way ensures that there is sufficient bandwidth for 2 operations writing from the primary OSD to the two replica OSDs while a reconstruction operation is takin place.


For an environment where the networks are all sharing the same physical paths, but segmented using VLANs, the calculation would be similar.
[.text-center]
[source]
[aggregate backup performance required] = [backup throughput required] * 4

[.text-left]

== RBD/CephFS Deployment
This section outlines the steps required to deploy an environment similar in architecture to the tested environment.

=== Deploy and prepare SUSE Enterprise Storage Environment

Build and deploy a SUSE Enterprise Storage Cluster as described in the SUSE Enterprise Storage Deployment Guide (https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_deployment/data/book_storage_deployment.html)

* Create an EC profile from command line on the admin node
[source]
ceph osd erasure-code-profile set veeam_ec plugin=isa k=4 m=2


==== Create Pools

* Create one pool for each protocol being supported
To create an EC pool
[source]
ceph osd pool create ecpool 512 512 erasure veeam_ec


* Create the RBD
[source]
rbd create reppool/veeam -size 5T -data-pool ecpool

=== Create and configure Linux repository virtual machines

. Create virtual machines on ESX
  * Configure resource reservations
. Perform base linux install
+
SLES 12 SPx::
.. Select KVM Host install pattern
.. Unselect KVM Host from Software Selection on Summary Screen
.. Disable spectre/meltdown post install
+
* Information regarding diabling Spectre and Meltdown mitigations can be found here: https://www.suse.com/support/kb/doc/?id=7023480
* Disabling these mitigations on the storage nodes and the Linux target may result in enhanced performance.
.. Enable Multi-queue block IO
+
* Do this on the Ceph OSD nodes and the Linux target VM(s)
* Information on blk-mq on enabling it can be found here: https://www.suse.com/documentation/sles-12/book_sle_tuning/data/cha_tuning_io_scsimq.html
+
SLES 15 SPx::
//FIXME validate the following
.. Select the base server pattern
.. Select to disable mitigations during install
.. Disable Spectre/Meltdown
+
All::
.. Set network tuning parameters in /etc/sysctl.conf for SUSE Enterprise Storage nodes and Linux target(s) as found in <<appendix>>.
... Add repositories and packages for veeam
... Veeam requires perl modules be present for the Linux repository to function. These are detailed in: https://www.veeam.com/kb2216 
.. Modify /etc/ssh/sshd_config to enable Veeam service to work correctly
... https://www.veeam.com/kb1512
... Find the PasswordAuthentication parameter and set the value to yes
... Save and restart the sshd daemon
+
[source]
systemctl restart sshd.service

. To add the required perl-SOAP-Lite, the SDK repos will need to be added.
+
For SLES12SP3::
+
[source]
SUSEConnect -p sle-sdk/12.3/x86_64
zypper in perl-SOAP-Lite

For SLES15::
+
[source]
SUSEConnect -p PackageHub/15/x86_64
zypper in perl-SOAP-Lite

+
// FIXME toms: not sure where this exactly belongs to
The following script can be used to validate that all packages/perl modules are installed.  If any are missing, they should be added.
[source]
#!/bin/bash
for i in constant Carp Cwd Data::Dumper Encode Encode::Alias \
   Encode::Config Encode::Encoding Encode::MIME::Name Exporter \
   Exporter::Heavy File::Path File::Spec File::Spec::Unix \
   File::Temp List::Util Scalar::Util SOAP::Lite Socket Storable threads
do
 echo "Checking for perl $i;..."
 perldoc -lm $i >/dev/null
 perlpkgfound=$?
 if [ ! $perlpkgfound -eq 1 ]
 then
   echo Installed
 fi
done

. Add ceph-common to the Linux target
[source]
zypper in ceph-common 

. Add client key and ceph.conf to /etc/ceph From the admin node:
[source]
scp /etc/* root@vtarget:/etc/ceph/

. Edit /etc/ceph/rbdmap on the Linux Repository nodes and add the rbd.
[source]
RbdDevice Parameters
poolname/imagename  id=client,keyring=/etc/ceph/ceph.client.keyring
reppool/veeam       id=admin,keyring=/etc/ceph/ceph.client.admin.keyring

. Enable and start systemd rbdmap service
[source]
systemctl enable rbdmap
systemctl start rbdmap

. Mkfs.xfs the target
[source]
mkfs.xfs /dev/rbd0

. Add mount point
[source]
mkdir /veeam

. Add entry to fstab (include any tuning desired)
[source]
/dev/rbd0 /veeam xfs _netdev 1 1

. Mount the filesystem
[source]
mount -a

. Verify it mounted
[source]
mount

+
Output should be:
+
[source]
/dev/rbd0 on /veeam type xfs (rw,relatime,attr2,inode64,sunit=8192,swidth=8192,noquota,_netdev)
[source]

== Add Veeam Linux Repository 

1.	Within the Veeam Console, click Backup Infrastructure on the left-hand menu bar. Right-click on Backup Repositories followed by Add Backup Repository 

2.	Provide a friendly name to distinguish the multiple repositories.

3.	Choose a repository type and click next. 

4.	Click Add New and enter the details and click next.

5.	Click Add to add credentials that have Read, Write, and Execute permissions to the mounted storage location and the ability to execute Perl code, and click OK and then Finish.

6.	Ensure the credentials are selected and click next.

7.	Click Browse and select the path to the mounted RBD with the XFS filesystem, and then click Advanced to select Use per-VM backup files.

8.	Finish the process by selecting a Mount server (Veeam Backup Server or proxy) and enabling a vPower NFS service as desired and selecting Finish


=== Disable Multiple Streams

Multiple streams are designed to enhance performance for higher latency environments. It may be desireable to disable this for the local deployment. This can be done when defining the job, by setting it for the proxy, or globally. In all cases, it involves selecting the Network Traffic Rules and de-selecting Multiple Streams.


=== Define a Backup Job

Create a backup job. When on the Storage setting tab, select the correct proxy and repository. While on the Storage screen, select Advanced. On the Storage tab, set the appropriate rules for your environment. Veeam Ready testing was performed using the information shown at the right.


== S3 Environment for Scale Out Backup Repository

Object storage repositories augment your scale-out backup abilities. It simplifies offloading existing backup data directy to cloud-based object storage. In our case, Veeam can leverage {SUSEProduct} to offload to S3 compatible environments such as Amazon S3, Microsoft Azure Blob Storage, IBM Cloud Object Storage. 

There are a relatively few number of steps when it comes to configuring {SUSEProduct} as an S3 target for a {Vendor} {Vplatform1}.

SES Preperation::
The following will need to be completed to prepare {SUSEProduct} 

* Install Rados Gateway 
- You can add a Rados Gateway role to an existing Monitor node or dedicated node for larger environments (recommended). In our case, we used a monitor named "example.ses5". This name will be specific to the name you set for your rados gateway. Please see the https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/salt_adding_services.html[Rados Gateway Installation Guide] for more information if needed.

// insert screenshot of policy.cfg

. Navigate to /srv/pillar/ceph/proposals/policy.cfg and match the existing host with a new role.
[source]
root@master # role-rgw/cluster/example.ses5.sls

. Run stage 2 to update the pillar
[source]
root@master # salt-run state.orch ceph.stage.2

. After making these custom changes, you should run stage 3 and 4 to apply the updates. Please see the https://www.suse.com/documentation/suse-enterprise-storage-5/pdfdoc/book_storage_admin/book_storage_admin.pdf[{Suseproduct} Guide for additional details if needed.]
[source]
root@master # salt-run state.orch ceph.stage.3
root@master # salt-run state.orch ceph.stage.4

=== Install and configure the RGW daemons

* IMPORTANT: Ensure that HTTPS/SSL is enabled on the target pool to allow {v} {vplatform1} to connect. This allows for secure communication supported by {v}. Please see the following section for https://www.suse.com/documentation/suse-enterprise-storage-5/book_storage_admin/data/ceph_rgw_https.html[enabling HTTPS/SSL for Object Gateways].

* You will need to modify the "rgw.conf" to allow port 443 (or 80 + 443). Navigate to the 'srv/salt/ceph/configuration/files/ceph.conf.d' directory to edit the rgw.conf file. 
[source]
root@master # cd /srv/salt/ceph/configuration/files/ceph.conf.d
root@master # vi rgw.conf

* Edit the contents of this file with the appropriate information listed below. The following represents what was used in the Veeam Ready testing process (parameter values below will vary):
+
[source]
....
[client{{ client }}]
rgw frontends = "civetweb port=80+443s ssl_certificate=/etc/ceph/rgw.pem"
rgw dns name = {{ fqdn }}
rgw enable usage log = false
rgw thread pool size = 512
rgw ops log rados = false
rgw max chunk size = 4194304
rgw num rados handles = 4
rgw usage max user shards = 4
rgw cache lru size = 100000
....
+

* Validate the Rados Gateway is in "active (running)" state by running systemctl, in our example the rados gateway is called "example.ses5". Please use the name of your rados gateway. 
[source]
systemctl status ceph-radosgw@example.ses5

// * You can verify this has worked properly by accessing the {SUSEproduct} (Open Attic) dashboard. You should see the Rados Gateway listed under the "Nodes" tab.  


=== Configure the Storage Pools

* Storage pools need to be created to host the {vplatform1} data. We must create a Ceph Object Pool for the Rados Gateway. You can do this via Open Attic dashboard or command line. The dashboard can help with your pg calculation, in our example we use 2048 (depends on environment). Examples of both are as follows:


image::{imgpath}SES_pool.png[SES Pool, scaledwidth=100%] 

* Create an erasure code profile from command line on the admin node

[source]
ceph osd erasure-code-profile set veeam_ec plugin=isa k=4 m=2

* Create the required pools

[source]
ceph osd pool create default.rgw.veeam.data 2048 2048 erasure veeam_ec
ceph osd pool create default.rgw.veeam.index 2048 2048 erasure veeam_ec
ceph osd pool create default.rgw.veeam.non-ec 2048 2048 replicated

==== Creating an S3 user

* When accessing the Object Gateway through the S3 interface you need to create an S3 user by running the below command and adjusting the options in <> brackets. This can also be done using the Open Attic dashboard by going to 'Object Gateway > User' tab. 
[source] 
root@mater # radosgw-admin user create --uid=<username> \
--display-name=<display-name> --email=<email>


* Configure a placement policy and set user placement

[source]
....
radosgw-admin zonegroup placement add --rgw-zonegroup default --placement-id veeam

radosgw-admin zone placement add --rgw-zone default --placement-id veeam --data-pool default.rgw.veeam.data --index-pool default.rgw.veeam.index --data-extra-pool default.rgw.veeam.non-ec

radosgw-admin metadata get user:veeam > user.json
....

* Edit the user.json and change default_placement to the placement-id created
[source]
"default_placement":"veeam"

* Next, save the changes and commit them
[source] 
radosgw-admin metadata put user:<user-id> <user.json

=== {v} {vplatform1} Configuration
After succesfully completing the steps above for {SUSEproduct} preperation, you can proceed to properly configuring {v} {vplatform1}. {v} documents this process very well and you can follow step-by-step instructions via https://helpcenter.veeam.com/docs/backup/vsphere/new_object_storage.html?ver=95u4[this link to their help center site.]

Tips and reminders::
* {v} will prompt you for a service point - use the IP of the gateway node
* Provide the access and secret keys, which can be found in 'Open Attic > Object > user' tab.
* The {v} software wizard may ask for a self-signed certificate. You will receive an error if the self-signed certificate is not properly imported to {v} Server.
// insert screenshot of failure of connection
* A bucket can be created with Open Attic dashboard with the correct certificate or it can be create from the S3 browser once a user is created with the correct access/secret keys
* Please verify the connection to your bucket by logging into any S3 compatible browser from your Windows machine. You will be prompted for the new S3 user access and secret keys. 
//mention successful login


== {vplatform1} Tuning Parameters 

* In our testing, we met {v}'s performance requirements with several tuning parameters. Depending on workload and infrastructure this can be different for everyone. 
* To tune {vplatform1} return to the ceph.conf file found in '/srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf'.  The following represents what was used in our Veeam Ready testing process (parameter values below will vary):
+
[source]
....
[client.{{ client }}]
rgw frontends = "civetweb port=80+443s ssl_certificate=/etc/ceph/rgw.pem error_log_file=/var/log/ceph/dl360-3.rgw.error.log"
#rgw frontends = "beast port=80 ssl_port=443 ssl_certificate=/etc/ceph/rgw.pem"
rgw dns name = {{ fqdn }}
rgw enable usage log = false
rgw thread pool size = 512
rgw max chunk size = 4194304
#abhi changes
rgw_obj_stripe_size = 4194304 # (default 4M for luminous)
rgw_list_bucket_min_readahead = 4000 #(default 1000)
rgw_max_listing_results = 4000
rgw_cache_expiry_interval = 1800 #(default 900s)
rgw_enable_usage_log = false
rgw_enable_ops_log = false
rgw dynamic resharding = false
rgw override bucket index max shards = 50 # alternatively we reshard the bucket manually after creation
rgw bucket index max aio = 16 # default 8
rgw cache lru size = 50000
# GC settings
rgw_gc_obj_min_wait = 21600 #(default 2_hr), decreasing will more actively purge objects
rgw gc processor period = 7200 #(default 1hr, decreasing more actively purges deletion)
rgw objexp gc interval = 3600 # default 10_min, we dont run swift objexp. so no need to run this 
objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576 
....

* This configuration will then need to be pushed out to all Rados Gateways that may be running in {SUSEproduct} environment. 
+
[source]
salt 'salt_master_hostname' state.apply ceph.configuration.create
salt '*' state.apply ceph.configuration

== Special Notes

If performing numerous tests, it may be necessary to go to the Linux target host via SSH and run fstrim for the mounted filesystem to force garbage collection instead of allowing the default actions of lazy garbage collection.

In the Veeam Ready Testing, a performance issue was encountered when using SSDs directly on the ESX server as a restoration target. In this case, it was necessary to disable VAAI for VMware to perform optimally.  This was directly related to the particular storage being utilized to host the VMware virtual machines and is not related to SUSE Enterprise Storage 

* This is to resolve a problem that may occur with VMware writing to images hosted on VMFS. The symptom that indicates this may be needed is when iostat -xmt 1 on OSD nodes and on vtarget indicate very low utilization during a Veeam restore operation. https://kb.vmware.com/s/article/1033665
* In the Veeam Ready testing performed, adjusting this parameter enabled the restore tests to reach about 100MB/s per guest being restored. The limitation in that case was the device being written to.


== Conclusion
{vendor} {vplatform} represents a strong option for data center backup when combined with {SUSEProduct}.  The benefits to customers include increased efficiency and performance, while achieving industry leading cost efficiency.


++++
<?pdfpagebreak?>
++++

[appendix]
[[appendix]]
== Appendix A: OS Networking Configuration
[source] 
net.ipv4.ip_forward = 0 
net.ipv6.conf.all.forwarding = 0 
net.core.netdev_max_backlog = 10000 
net.core.netdev_budget = 300 
net.core.somaxconn = 128 
net.core.busy_poll = 50 
net.core.busy_read = 50 
net.core.rmem_max = 125829120
net.core.wmem_max = 125829120 
net.core.rmem_default = 125829120 
net.core.wmem_default = 125829120 
net.ipv4.tcp_fastopen = 1 
net.ipv4.tcp_low_latency = 1 
net.ipv4.tcp_sack = 1 
net.ipv4.tcp_rmem = 10240 87380 125829120 
net.ipv4.tcp_wmem = 10240 87380 125829120
net.ipv4.ip_local_port_range = 1024 64999 
net.ipv4.tcp_max_syn_backlog = 1024 
net.ipv4.tcp_tw_reuse = 0 
net.ipv4.tcp_tw_recycle = 0 
net.ipv4.tcp_timestamps = 0 
net.ipv4.tcp_syn_retries = 5 



++++
<?pdfpagebreak?>
++++

[appendix]
== Resources
•	Veeam KB – SUSE KB Articles
https://www.veeam.com/kb_search_results.html?product=Backup_Replication&kb-search-type=&search=suse
•	Veeam Documentation 
https://www.veeam.com/documentation-guides-datasheets.html
•	SUSE Enterprise Sftorage Technical Overview
https://www.suse.com/docrep/documents/1mdg7eq2kz/suse_enterprise_storage_technical_overview_wp.pdf 
•	SUSE Enterprise Storage v5 – Deployment Guide
https://www.suse.com/documentation/suse-enterprise-storage-5/pdfdoc/book_storage_deployment/book_storage_deployment.pdf 
•	SUSE Enterprise Storage v5 – Administration Guide
https://www.suse.com/documentation/suse-enterprise-storage-5/pdfdoc/book_storage_admin/book_storage_admin.pdf 
